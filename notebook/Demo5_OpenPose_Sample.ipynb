{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demo5_OpenPose_Sample.ipynb","provenance":[],"collapsed_sections":["HrqOURo4iFRe","4N9chuQNiHq1","td0ETIPIihhy","DYzoPv_Giwtf","fPEr3LY8gySK"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sCvrUw3Tw_UI","colab_type":"text"},"source":["### 事前準備"]},{"cell_type":"markdown","metadata":{"id":"NUJW5Go6Dyyr","colab_type":"text"},"source":["**TensorFlow1.xのセットアップ**\n","ColabのDefault Versionが2.0になった時の対応用。<font color=red>**ランタイムの再起動が必要!!**</font>"]},{"cell_type":"code","metadata":{"id":"-eInSU6uDzhJ","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJNfBZ6LxVWn","colab_type":"text"},"source":["GoogleDriveのマウント"]},{"cell_type":"code","metadata":{"id":"5HtLv_RlS7D1","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"miNnXaZP0txA","colab_type":"text"},"source":["PC-Cameraの準備"]},{"cell_type":"code","metadata":{"id":"sl3bOtAD0xtX","colab_type":"code","colab":{}},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mn8zZ18PjX0q","colab_type":"text"},"source":["# **Open Poseを使った骨格推定（既成のモデルを使った手軽なAI）**\n"]},{"cell_type":"markdown","metadata":{"id":"QRmcJu6wJflM","colab_type":"text"},"source":["<table><tr>\n","<td><img src=\"https://www.fujitsu.com/jp/imagesgig5/pict_38_tcm102-4508765_tcm102-2750236-32.png\" width=\"100\"></td>\n","<td><h1><b>【ご参考】Zinraiによる行動分析ソリューション</b></h1></td>\n","</tr></table> \n","\n","**<font color=red>※OpenPoseでは無いが、Zinraiでも骨格推定を用いた行動分析ソリューションを提供しています。</font>**\n","\n","> [人の映像から、動作や行動をディープラーニングで認識します。\n","「歩く」「首を振る」「手を伸ばす」など約100種類の基本動作を90%以上の精度で認識する学習済みモデルを用い、これら基本動作を組み合わせることで、不審行動や購買行動といった人の複雑な行動を認識させることが可能です。](https://www.fujitsu.com/jp/solutions/business-technology/ai/ai-zinrai/technology/index.html#img-03)   \\\\\n"]},{"cell_type":"markdown","metadata":{"id":"W3wKhIsgE1gM","colab_type":"text"},"source":["### OpenPoseとは\n","> OpenPoseは、コンピュータビジョンに関する国際学会CVPR2017でCMU（カーネギーメロン大学）が発表した、keypoint（特徴点）の検出とkeypoint同士の関係の推定を行う技術です。OpenPoseを使うと、関節の位置など、人の体における特徴点がどの座標にあるかが分かります。"]},{"cell_type":"markdown","metadata":{"id":"_JsjUTDCGGQv","colab_type":"text"},"source":["### **セットアップ(OpenPose)**"]},{"cell_type":"markdown","metadata":{"id":"XbiBNSc90HXB","colab_type":"text"},"source":["OpenPoseをセットアップする。（だいたい 3～4分くらいかかる）"]},{"cell_type":"code","metadata":{"id":"zlu4FJNPjlTi","colab_type":"code","colab":{}},"source":["# SWIG を準備\n","!apt-get -q -y install swig\n","# TF-openposeをクローン\n","!git clone https://www.github.com/ildoonet/tf-openpose\n","# クローンしたディレクトリに入る\n","%cd tf-openpose\n","# openpose動作のための、ライブラリをインストール\n","!pip3 install -r requirements.txt\n","# Openposeのモデルをダウンロード\n","%cd models/graph/cmu\n","!bash download.sh\n","%cd ../../../\n","# pafprocessをインストール\n","%cd tf_pose/pafprocess\n","!ls\n","!swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n","%cd ../../"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MoNcL8qFEiWv","colab_type":"text"},"source":["run.pyの若干カスタマイズ版run_new.py(FileSaveを追加)をGoogleDriveからコピー"]},{"cell_type":"code","metadata":{"id":"WYnWi1a5Eytn","colab_type":"code","colab":{}},"source":["!cp \"/content/drive/My Drive/Colab Notebooks/run_new.py\" \"/content/tf-openpose/run_new.py\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNfvslEs6uya","colab_type":"text"},"source":["## **写真画像から骨格を推定**"]},{"cell_type":"markdown","metadata":{"id":"9W-FLaH4J1i1","colab_type":"text"},"source":["**PC-Cameraを起動**"]},{"cell_type":"code","metadata":{"id":"AAyGHWY00540","colab_type":"code","colab":{}},"source":["from IPython.display import Image\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","  display(Image(filename))\n","except Exception as err:\n","  print(str(err))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STahMRgIJ8eR","colab_type":"text"},"source":["**撮影画像から骨格を推定(OpenPose)**"]},{"cell_type":"code","metadata":{"id":"3OqYNQeZpfUC","colab_type":"code","colab":{}},"source":["#%run -i run.py --model=mobilenet_thin --image=/content/tf-openpose/photo.jpg\n","#!cd /content/tf-openpose\n","%run -i run_new.py --model=mobilenet_thin --image=/content/tf-openpose/photo.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiSCj8pPqua0","colab_type":"code","colab":{}},"source":["  # 骨格推定結果の表示（参考）\n","  display(Image(\"/content/tf-openpose/result.jpg\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7veNbxLiqEgI","colab_type":"code","colab":{}},"source":["!cd /content/tf-openpose/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HrqOURo4iFRe"},"source":["# 番外：動画からOpenPoseで骨格推定"]},{"cell_type":"markdown","metadata":{"id":"gCSK-Igshfq0","colab_type":"text"},"source":["**ちょーおおまかな概要** \\\\\n","> 動画データから1枚ずつフレームを切り出し、切り出した画像を一枚ずつ骨格推定して、最後にそれをつなぎ合わせて動画にする。。。 \\\\\n","サンプルは、YouTubeから拝借。。（Zinrai関連＝ノクリアCM、富士通チア、エフサスTimeCreatorのCM）\n"]},{"cell_type":"markdown","metadata":{"id":"xDjWHAoCe6D0","colab_type":"text"},"source":["**サンプル動画の準備**：GoogleDriveに予め用意した動画を、Colabの/tf-openpose配下にコピー"]},{"cell_type":"code","metadata":{"id":"DxL0GaG3qESG","colab_type":"code","colab":{}},"source":["!cp '/content/drive/My Drive/dance.mp4' \"/content/tf-openpose/dance.mp4\"\n","!cp '/content/drive/My Drive/FSAS_TC_30S.mp4' \"/content/tf-openpose/FSAS_TC_30S.mp4\"\n","!cp '/content/drive/My Drive/FJ_Cheer.mp4' \"/content/tf-openpose/FJ_Cheer.mp4\"\n","!cp '/content/drive/My Drive/FJ_ZinraiCM.mp4' \"/content/tf-openpose/FJ_ZinraiCM.mp4\"\n","!pwd\n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VrFADbGDuji8","colab_type":"text"},"source":["**動画の確認（再生）**：うまく再生できない時がある（その時はGoogleDriveの元動画をプレビュー）"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ziu44_OuheTD","colab":{}},"source":["from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('/content/tf-openpose/Out_dance.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=600 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJftrKEDfWCi","colab_type":"text"},"source":["**動画で骨格推定し動画で保存**：動画で骨格推定し、/tf-openpose配下にOut_dance.mp4で保存"]},{"cell_type":"markdown","metadata":{"id":"4N9chuQNiHq1","colab_type":"text"},"source":["##### **Sample1：富士通エフサス TimeCreatorのCM動画**"]},{"cell_type":"code","metadata":{"id":"P4WcS-NAqEBC","colab_type":"code","colab":{}},"source":["import argparse\n","import logging\n","import time\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","movie_name = 'FSAS_TC_30S'\n","img_outdir = './img'\n","os.makedirs(img_outdir, exist_ok=True)\n","fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')  # 動画作成\n","video  = cv2.VideoWriter('Out_dance.mp4', fourcc, 10.0, (1280, 720))\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='tf-pose-estimation Video')\n","    outimg_files = []\n","    count = 0\n","    w = 640 \n","    h = 360\n","    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(w, h))\n","    cap = cv2.VideoCapture('FSAS_TC_30S.mp4')       # 動画読み込み\n","    while True:         # 動画用の画像作成\n","        ret, image = cap.read()\n","        if ret == True:\n","            count += 1                # １フレームずつ処理\n","            if count % 100 == 0:\n","                print('Image No.：{0}'.format(count))\n","            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4)\n","            image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","            outimg_file = '{}/{:05d}.jpg'.format(img_outdir, count)               # 画像出力\n","            cv2.imwrite(outimg_file, image)\n","            video.write(image)       \n","        else:\n","            break\n","    video.release()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"td0ETIPIihhy","colab_type":"text"},"source":["##### **Sample2：富士通ゼネラルのノクリアCM（ノクリア Powered by Zinrai）の動画**"]},{"cell_type":"code","metadata":{"id":"pj7ANEPJ7A2m","colab_type":"code","colab":{}},"source":["import argparse\n","import logging\n","import time\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","movie_name = 'FJ_ZinraiCM'\n","img_outdir = './img'\n","os.makedirs(img_outdir, exist_ok=True)\n","fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')  # 動画作成\n","video  = cv2.VideoWriter('Out_dance.mp4', fourcc, 10.0, (640, 360))\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='tf-pose-estimation Video')\n","    outimg_files = []\n","    count = 0\n","    w = 640 \n","    h = 360\n","    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(w, h))\n","    cap = cv2.VideoCapture('FJ_ZinraiCM.mp4')       # 動画読み込み\n","    while True:         # 動画用の画像作成\n","        ret, image = cap.read()\n","        if ret == True:\n","            count += 1                # １フレームずつ処理\n","            if count % 100 == 0:\n","                print('Image No.：{0}'.format(count))\n","            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4)\n","            image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","            outimg_file = '{}/{:05d}.jpg'.format(img_outdir, count)               # 画像出力\n","            cv2.imwrite(outimg_file, image)\n","            video.write(image)       \n","        else:\n","            break\n","    video.release()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYzoPv_Giwtf","colab_type":"text"},"source":["##### **Sample3：富士通フロンティアーズのチア動画**"]},{"cell_type":"code","metadata":{"id":"P4dIX_a6i4Xz","colab_type":"code","colab":{}},"source":["import argparse\n","import logging\n","import time\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","movie_name = 'FJ_Cheer'\n","img_outdir = './img'\n","os.makedirs(img_outdir, exist_ok=True)\n","fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')  # 動画作成\n","video  = cv2.VideoWriter('Out_dance.mp4', fourcc, 10.0, (426, 240))\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='tf-pose-estimation Video')\n","    outimg_files = []\n","    count = 0\n","    w = 640 \n","    h = 360\n","    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(w, h))\n","    cap = cv2.VideoCapture('FJ_Cheer.mp4')       # 動画読み込み\n","    while True:         # 動画用の画像作成\n","        ret, image = cap.read()\n","        if ret == True:\n","            count += 1                # １フレームずつ処理\n","            if count % 100 == 0:\n","                print('Image No.：{0}'.format(count))\n","            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=4)\n","            image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","            outimg_file = '{}/{:05d}.jpg'.format(img_outdir, count)               # 画像出力\n","            cv2.imwrite(outimg_file, image)\n","            video.write(image)       \n","        else:\n","            break\n","    video.release()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8A60vTQxf6A-","colab_type":"text"},"source":["**骨格推定した動画をGoogleDriveにコピー**：結果を確認するために、GoogleDriveにコピーする \\\\\n","※ファイル反映にラグあり。何度かコピーしてcolabのエクスプローラで反映されているか確認する※"]},{"cell_type":"markdown","metadata":{"id":"WEIRrRrNjU71","colab_type":"text"},"source":["##### **骨格推定した動画の確認（再生）**：GoogleDriveにコピーして、動画を再生。<font color=red><b>（GoogleDriveの方からプレビューする）</b></font>"]},{"cell_type":"code","metadata":{"id":"Rk8U7jrxa705","colab_type":"code","colab":{}},"source":["!cp \"/content/tf-openpose/Out_dance.mp4\" \"/content/drive/My Drive/Out_dance.mp4\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZee09ftdBIL","colab_type":"text"},"source":["**<後処理>ワークファイル(/tf-openpose/img配下の骨格推定後フレームjpeg)の削除**"]},{"cell_type":"code","metadata":{"id":"oW3WJNWOVwg5","colab_type":"code","colab":{}},"source":["!rm -r /content/tf-openpose/img/*\n","!ls -x /content/tf-openpose/img"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPEr3LY8gySK","colab_type":"text"},"source":["# 番外：PoseNetというのもあります（ご参考）"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ryq9nYuMijuP"},"source":["**PoseNet** \\\\\n","> Googleが開発したWebブラウザでリアルタイムに人間の姿勢推定を可能にする機械学習モデル。　 \\\\\n","映像中の人物から1つのポーズまたは複数のポーズを検出できる。   \\\\\n","詳細は、https://github.com/tensorflow/tfjs-models/tree/master/posenet"]},{"cell_type":"markdown","metadata":{"id":"C08hqnq0iHtQ","colab_type":"text"},"source":["JavaScriptで動作し、WebCamも使える。公式サイトでデモ公開しているので、ちょっとみましょう！ \\\\\n","**[デモサイト](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html)**\n"]}]}