{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yana-testing_Keras-3-NaturalLanguageClassifier_livedoorSample.ipynb","version":"0.3.2","provenance":[{"file_id":"1KWN1B2Ah3WmcCPLleOpy11pwxK5PGOdf","timestamp":1552747287038}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"otSQOdbbh19C","colab_type":"text"},"cell_type":"markdown","source":["###### ドライブ共有準備"]},{"metadata":{"id":"QMtVmrBgXod4","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('./gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FUClW2R7dGyz","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","sys.getdefaultencoding()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QtSeRcCOSRJN","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n"]},{"metadata":{"id":"jWBjfEgV2iQm","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"LCh8DIGyR47k","colab_type":"text"},"cell_type":"markdown","source":["# Kerasで文章分類器を作成\n","*   Kerasはtensorflowなどで動くPythonで書かれたニューラルネットワークライブラリ。詳細は[こちら](https://keras.io/ja/)。\n","*   データは「livedoor ニュースコーパス」を使用。各記事ファイルにはクリエイティブ・コモンズライセンス「表示 – 改変禁止」が適用される。詳細はフォルダのLICENSE.txt参照。\n"]},{"metadata":{"id":"9LMRsQkQ3EO4","colab_type":"text"},"cell_type":"markdown","source":["### データの事前展開\n","> Google DriveからColabへのファイルコピー"]},{"metadata":{"id":"diOZ_JZOIX_L","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -U -q PyDrive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l2xKYYaBRD7K","colab_type":"code","colab":{}},"cell_type":"code","source":["# Google Driveアクセス準備\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ozHqkIkwSsyl","colab_type":"code","colab":{}},"cell_type":"code","source":["# Google Driveからのコピー\n","id = '1BwbaNc3TJu5ENE36n3EOty8a-Z6XXe33'  # 共有リンクで取得した id= より後の部分(ldcc-20140209.tar.gzの例)\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('ldcc-2014-2-9.zip')\n","# 終わったら、ファイル画面の更新をして確認"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gcnp1f7FT77o","colab_type":"code","colab":{}},"cell_type":"code","source":["# tarの展開（中身はZipじゃないみたいなので展開だけでOK）\n","# 終わったら、ファイル画面の更新をして/textがあればOK\n","#!tar -xvf ldcc-20140209.tar.gz\n","!unzip ldcc-2014-2-9.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8hJU69c7cI2E","colab_type":"text"},"cell_type":"markdown","source":["### ツールの準備（janome）"]},{"metadata":{"id":"-JQexxNCCU8U","colab_type":"code","colab":{}},"cell_type":"code","source":["# janomeインストール：形態素解析エンジン。形態素の分割、品詞判定、分かち書きができる\n","!pip install janome"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fFHbr3OkSBpv","colab_type":"code","colab":{}},"cell_type":"code","source":["# janomeの動作確認\n","from janome.tokenizer import Tokenizer\n","\n","# テスト\n","word = Tokenizer().tokenize('患者さんは診察後に支払窓口で会計をすることなく、帰宅が可能となります')\n","for i in range(len(word)):\n","    print(i,word[i])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Jzi6osPOEBVR"},"cell_type":"markdown","source":["### データ準備（ベクトル化）"]},{"metadata":{"id":"6yUDs1hwITQh","colab_type":"text"},"cell_type":"markdown","source":["日本語テキストを機械学習に入力するため、テキストを数値化（ベクトル化）する。\n","\n","ベクトル化する前に、**janomeで「分かち書き」したデータを作る**　\n","\n","※分かち書きとは、英語のような「語を空白で区切ったもの」。今回はシンプルに名詞のみ抽出し、その他は捨てる。\n"]},{"metadata":{"id":"jHK7p_SgG0v8","colab_type":"code","colab":{}},"cell_type":"code","source":["from janome.tokenizer import Tokenizer\n","import os, glob\n"," \n","# Janomeを使って形態素解析\n","ja_tokenizer=Tokenizer()\n"," \n","# 分かち書きし、日本語文から名詞のみ抽出する\n","def ja_tokenize(text):\n","    res=[]\n","    lines=text.split(\"\\n\")\n","    lines=lines[3:] # サンプルデータの最初の3行はヘッダーと題名なので捨てる\n","    for line in lines:\n","        malist=ja_tokenizer.tokenize(line)\n","        for tok in malist:\n","            ps=tok.part_of_speech.split(\",\")[0]\n","            # 名詞でなければ以下の処理をスキップ\n","            if not ps in ['名詞']: continue               \n","            w=tok.base_form\n","            if w==\"*\" or w==\"\": w=tok.surface\n","            if w==\"\" or w==\"\\n\": continue\n","            res.append(w)\n","        res.append(\"\\n\")\n","    return res\n"," \n","# テキストデータを読み込み\n","dir ='./text'\n","for path in glob.glob(dir + \"/*/*.txt\", recursive=True):\n","    # LICENSE.txtは以下の処理をスキップ\n","    if path.find(\"LICENSE\")>0:continue          \n","    #print(path)\n","    path_wakati=path + \".wakati\"\n","    # 既に \"wakati\"ファイルがあれば以下の処理をスキップ\n","    if os.path.exists(path_wakati):continue        \n","    text=open(path,\"r\", encoding='utf-8').read() \n","    words=ja_tokenize(text)\n","    wt=\" \".join(words)\n","    open(path_wakati, \"w\", encoding=\"utf-8\").write(wt)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"74mEkEgBpOam","colab_type":"code","colab":{}},"cell_type":"code","source":["# 分かち書き処理の確認\n","print(\"---------- ↓↓↓↓ 分かち書き前 ↓↓↓↓ ------------\")\n","from __future__ import with_statement\n","with open('./text/0/dokujo-tsushin-4778031.txt', 'r') as f:\n","    print(f.read())\n","print(\"---------- ↓↓↓↓ 分かち書き後 ↓↓↓↓ ------------\")\n","from __future__ import with_statement\n","with open('./text/0/dokujo-tsushin-4778031.txt.wakati', 'r') as f:\n","    print(f.read())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L-0ky2NXK6H_","colab_type":"code","colab":{}},"cell_type":"code","source":["# 分かち書きしたテキストを数値データに変換  ※15-16分かかる※\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import os, glob\n","\n","dic={} # 辞書初期化\n","count = 0  # 辞書登録用カウンタ\n","folder = [\"0\",\"1\",\"2\",\"3\",\"4,\",\"5\",\"6\",\"7\",\"8\"]\n","#folder = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\",\"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\"]\n","x, y = [], []\n","\n","for index, name in enumerate(folder):\n","    dir = \"./text/\" + name\n","    files = sorted(glob.glob(dir + \"/*.wakati\"))  # ソートして読み込み\n","    for i, file in enumerate(files):\n","        with open(file, \"r\", encoding='utf-8') as f:\n","             text = f.read()\n","             text = text.strip()\n","             text = text.split(\" \")\n","             result = []  # 単語を数字化した結果を入れるリスト\n","             for word in text:\n","                 word = word.strip()  \n","                 if word == \"\": continue\n","                 if not word in dic: # 未登録の場合\n","                    dic[word] = count  # count の数字で辞書に登録\n","                    num = count\n","                    count +=1\n","                    #print(num,word)  # 数字と単語を表示\n","                 else:\n","                    num=dic[word] # 数字を辞書で調べる\n","                 result.append(num)  # リストに数字を追加\n","             x.append(result)  # Xの作成：文書に含まれる名詞を数値(辞書内の番号)に置き換えたデータ。名詞と数値の対応関係は辞書(dic)で管理。\n","             y.append(index)   # Yの作成：文書のカテゴリ番号を追加。0 はdokujo-tsushin, 1 はit-life-hackなど\n","\n","             \n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=111)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')\n","\n","num_classes = np.max(y_train) + 1\n","print(num_classes, 'classes')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TsYW_DEnFa1W","colab_type":"text"},"cell_type":"markdown","source":["＜参考＞ベクトル化したデータの確認"]},{"metadata":{"id":"USmP1RvUtsvH","colab_type":"code","colab":{}},"cell_type":"code","source":["# 2番目のテキスト文書の表示（ベクトル化 前）\n","from __future__ import with_statement\n","with open('./text/0/dokujo-tsushin-4778031.txt.wakati', 'r') as f:\n","    print(f.read())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T1ky4Sett8wh","colab_type":"code","colab":{}},"cell_type":"code","source":["# 先頭の単語に対応した辞書登録番号(数字)を表示\n","print(dic['携帯'])\n","print(dic['電話'])\n","print(dic['普及'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kgE_z6nBoc4S","colab_type":"code","colab":{}},"cell_type":"code","source":["# ベクトル化した文書を表示\n","print(x[1])  # 2番目(0,1,2,...)のテキスト(dokujo-tsushin-4778031.txt.wakati)を数値化したもの\n","print(y[1])  # 2番目(0,1,2,...)のテキストのカテゴリ番号(dokujo-tsushinは0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yetynefLwYZS","colab_type":"code","colab":{}},"cell_type":"code","source":["# （参考）辞書に登録した単語と登録番号の内容\n","print(dic)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Kh3M5kFD2-2W","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train[0]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"3XSftveYGgAW"},"cell_type":"markdown","source":["### 学習モデルの構築"]},{"metadata":{"id":"apZToH6oqen1","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import print_function\n","import numpy as np\n","import keras\n","import glob\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","max_words = 1000  # ワード数を増やすと精度が若干かわる\n","batch_size = 32\n","epochs = 10\n","\n","print('-------分かち書き後のデータのベクトル化-------')\n","tokenizer = Tokenizer(num_words=max_words)\n","x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n","x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","print('y_train shape:', y_train.shape)\n","print('y_test shape:', y_test.shape)\n","\n","print('-------モデル構築-------')\n","model = Sequential()\n","model.add(Dense(512, input_shape=(max_words,)))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.save('MyNLCsam.h5')\n","history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_split=0.1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7nIJ3_U11Byc","colab_type":"text"},"cell_type":"markdown","source":["**モデルを評価**"]},{"metadata":{"id":"BsCQDqtP1MMF","colab_type":"code","colab":{}},"cell_type":"code","source":["score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)     # 損失値の計算\n","print('Test score:', score[0])       # 損失値(1-損失値)\n","print('Test accuracy:', score[1])    # 精度\n","model.save('MyNLCsam.h5')\n","\n","### 学習経過のプロット\n","import matplotlib.pyplot as plt\n","from IPython.display import Image,display_png\n","acc = history.history[\"acc\"]\n","val_acc = history.history[\"val_acc\"]\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, \"bo\", label = \"Training acc\" )\n","plt.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\n","plt.title(\"Training and Validation accuracy\")\n","plt.legend()\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_QNWKXAwdSVc","colab_type":"text"},"cell_type":"markdown","source":["### 推論のテスト"]},{"metadata":{"id":"kY9oepvI_53O","colab_type":"text"},"cell_type":"markdown","source":["とりあえず、ベクトル化したテキストが既に準備されている前提で、推論のテスト実施。"]},{"metadata":{"id":"MLDpeDhWWV6B","colab_type":"code","colab":{}},"cell_type":"code","source":["# 推論したい文章を入力\n","path_w = 'in-sample.txt'\n","sam = 'AIの安心・安全な利用に向けた「富士通グループAIコミットメント」を策定　\\\n","当社は、このたび、AIの安心・安全な利用に向けて、当社グループにおけるAI関連の技術や\\\n","ソリューション・サービスの研究開発に際し、AI倫理を含む価値観をまとめた、\\\n","「富士通グループAIコミットメント」を策定しました。\\\n","当社グループは、本コミットメントに基づき、「人と協調する、人を中心とするAI」を具現化した\\\n","技術やソリューション・サービスの提供をグローバルに推進していきます。'\n","\n","with open(path_w, mode='w') as f:    # 推論したいテキストを作成\n","    f.write(sam)\n","\n","print('---------推論文章（分かち書き前）------------')\n","with open(path_w) as f:\n","    print(f.read())\n","\n","\n","# ----分かち書きして、FileName.txt.wakatiで書き込み----\n","from janome.tokenizer import Tokenizer\n","import os, glob\n","ja_tokenizer=Tokenizer()    # 分かち書きし、日本語文から名詞のみ抽出する\n","\n","def ja_tokenize(text):\n","    res=[]\n","    lines=text.split(\"\\n\")\n","    for line in lines:\n","        malist=ja_tokenizer.tokenize(line)\n","        for tok in malist:\n","            ps=tok.part_of_speech.split(\",\")[0]\n","            # 名詞でなければ以下の処理をスキップ\n","            if not ps in ['名詞']: continue               \n","            w=tok.base_form\n","            if w==\"*\" or w==\"\": w=tok.surface\n","            if w==\"\" or w==\"\\n\": continue\n","            res.append(w)\n","        res.append(\"\\n\")\n","    return res\n"," \n","for path in glob.glob(path_w, recursive=True):\n","    path_wakati=path + \".wakati\"\n","    # 既に \"wakati\"ファイルがあれば以下の処理をスキップ\n","    if os.path.exists(path_wakati):continue        \n","    text=open(path,\"r\", encoding='utf-8').read() \n","    words=ja_tokenize(text)\n","    wt=\" \".join(words)\n","    open(path_wakati, \"w\", encoding=\"utf-8\").write(wt)\n","\n","print('')\n","print('---------推論文章（分かち書き後）------------')\n","with open(path_wakati) as f:\n","    print(f.read())\n","\n","\n","# ----分かち書きしたテキストを数値データに変換----\n","import numpy as np\n","import os, glob\n","#dic={} # 既存辞書に追加するので初期化しない\n","#count = 0  # 既存辞書登録用カウンタに追加するので初期化しない\n","x1 = []\n","for i, file in enumerate(files):\n","    with open(path_wakati, \"r\", encoding='utf-8') as f:\n","         text = f.read()\n","         text = text.strip()\n","         text = text.split(\" \")\n","         result = []  # 単語を数字化した結果を入れるリスト\n","         for word in text:\n","             word = word.strip()  \n","             if word == \"\": continue\n","             if not word in dic: # 未登録の場合\n","                dic[word] = count  # count の数字で辞書に登録\n","                num = count\n","                count +=1\n","             else:\n","                num=dic[word] # 数字を辞書で調べる\n","             result.append(num)  # リストに数字を追加\n","             x1.append(result)  # Xの作成：文書に含まれる名詞を数値(辞書内の番号)に置き換えたデータ。名詞と数値の対応関係は辞書(dic)で管理。\n","print('---------推論文章（ベクトル化後）------------')\n","print(x1)             \n","x1 = tokenizer.sequences_to_matrix(x1, mode='binary')\n","\n","\n","model.load_weights('MyNLCsam.h5')\n","print('')\n","print('---------！！推論結果！！------------')\n","pred = model.predict(x1, batch_size=1, verbose=0)\n","pred_score = np.max(pred)\n","pred_label = np.argmax(pred)\n","print('予測スコア:', pred_score * 100 , '%')       # \n","print('予測ラベル:', pred_label)    # "],"execution_count":0,"outputs":[]},{"metadata":{"id":"7IWcY8nzdyy6","colab_type":"text"},"cell_type":"markdown","source":["### （その他）メモ書きなど"]},{"metadata":{"id":"14wHHyjhRjSH","colab_type":"text"},"cell_type":"markdown","source":["*  notebook上で**デバッガー**を使う際は[ここを参照](https://recruit-tech.co.jp/blog/2018/10/16/jupyter_notebook_tips/#b3)"]},{"metadata":{"id":"IWbtElKwVjwr","colab_type":"code","colab":{}},"cell_type":"code","source":["#辞書型の使用例\n","def getValue(key, items):\n","    values = [x['Value'] for x in items if 'Key' in x and 'Value' in x and x['Key'] == key]\n","    return values[0] if values else None\n","\n","\n","# 使用例\n","items = [{'Key': 0,'Value': '独女通信'},\n","         {'Key': 1,'Value': 'ITライフハック'},\n","         {'Key': 2,'Value': '家電チャンネル'},\n","         {'Key': 3,'Value': 'Livedoor home'},\n","         {'Key': 4,'Value': 'movie-enter'},\n","         {'Key': 5,'Value': 'Peachy'},\n","         {'Key': 6,'Value': 'Smax'},\n","         {'Key': 7,'Value': 'スポーツウォッチ'},\n","         {'Key': 8,'Value': 'トピックニュース'}]\n","print(getValue(3, items))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zUdIt9ykQgjK","colab_type":"text"},"cell_type":"markdown","source":["---"]}]}